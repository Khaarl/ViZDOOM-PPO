{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup and Dependencies\n",
    "Install and configure required packages (vizdoom, stable-baselines3, etc). Set up logging and directory structure. Define configuration constants and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install -y build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
    "    nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
    "    libopenal-dev timidity libwildmidi-dev unzip ffmpeg\n",
    "\n",
    "!pip install vizdoom\n",
    "!pip install stable-baselines3[extra]\n",
    "\n",
    "# Set up logging and directory structure\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Define local paths for scenario and storage\n",
    "LOCAL_SCENARIO_PATH = \"/content/scenarios/deathmatch.cfg\"\n",
    "LOCAL_STORAGE_PATH = \"/content/scenarios/training_data\"\n",
    "LOCAL_MODEL_PATH = \"/content/scenarios/training_data/models\"\n",
    "LOCAL_LOG_PATH = \"/content/scenarios/training_data/logs\"\n",
    "LOCAL_TENSORBOARD_PATH = \"/content/scenarios/training_data/tensorboard\"\n",
    "LOCAL_WAD_PATH = \"/content/scenarios/freedoom2.wad\"\n",
    "\n",
    "# Create local directories\n",
    "os.makedirs(LOCAL_STORAGE_PATH, exist_ok=True)\n",
    "os.makedirs(LOCAL_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(LOCAL_LOG_PATH, exist_ok=True)\n",
    "os.makedirs(LOCAL_TENSORBOARD_PATH, exist_ok=True)\n",
    "print(\"Created local directories.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=os.path.join(LOCAL_LOG_PATH, 'setup.log'), level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Directories created and logging configured.\")\n",
    "\n",
    "# Download freedoom2.wad if it doesn't exist\n",
    "if not os.path.exists(LOCAL_WAD_PATH):\n",
    "    !wget https://github.com/freedoom/freedoom/releases/download/v0.13.0/freedoom2.wad -O $LOCAL_WAD_PATH\n",
    "    logging.info(f\"Downloaded freedoom2.wad to {LOCAL_WAD_PATH}\")\n",
    "else:\n",
    "    logging.info(f\"Using existing freedoom2.wad at {LOCAL_WAD_PATH}\")\n",
    "\n",
    "# Download deathmatch.cfg if it doesn't exist\n",
    "if not os.path.exists(LOCAL_SCENARIO_PATH):\n",
    "    !wget https://raw.githubusercontent.com/mwydmuch/ViZDoom/master/scenarios/deathmatch.cfg -P /content/scenarios/\n",
    "    logging.info(f\"Downloaded deathmatch.cfg to {LOCAL_SCENARIO_PATH}\")\n",
    "else:\n",
    "    logging.info(f\"Using existing deathmatch.cfg at {LOCAL_SCENARIO_PATH}\")\n",
    "\n",
    "# Define configuration constants and helper functions\n",
    "\n",
    "def get_user_input(prompt, type_=None, min_=None, max_=None, range_=None):\n",
    "    if min_ is not None and max_ is not None and max_ < min_:\n",
    "        raise ValueError(\"min_ must be less than or equal to max_.\")\n",
    "    while True:\n",
    "        val = input(prompt)\n",
    "        if type_ is not None:\n",
    "            try:\n",
    "                val = type_(val)\n",
    "            except ValueError:\n",
    "                print(f\"Input must be of type {type_.__name__}.\")\n",
    "                continue\n",
    "        if min_ is not None and val < min_:\n",
    "            print(f\"Input must be greater than or equal to {min_}.\")\n",
    "        elif max_ is not None and val > max_:\n",
    "            print(f\"Input must be less than or equal to {max_}.\")\n",
    "        elif range_ is not None and val not in range_:\n",
    "            if isinstance(range_, range):\n",
    "                template = f\"Input must be between {range_.start} and {range_.stop-1}.\"\n",
    "            else:\n",
    "                template = f\"Input must be {', '.join(map(str, range_))}.\"\n",
    "            print(template)\n",
    "        else:\n",
    "            return val\n",
    "\n",
    "logging.info(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ViZDoom Environment Class\n",
    "Implement custom ViZDoom environment class inheriting from gym.Env. Handle observation/action spaces, reward shaping, state management and game lifecycle. Include unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ViZDoom Environment Class\n",
    "\n",
    "from vizdoom import *\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import os\n",
    "from google.colab import drive\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Mount Drive and Setup Paths\n",
    "drive.mount('/content/drive')\n",
    "BASE_PATH = \"/content/drive/MyDrive/ViZDoom-PPO\"\n",
    "os.makedirs(f\"{BASE_PATH}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{BASE_PATH}/logs\", exist_ok=True)\n",
    "\n",
    "class VizdoomEnv(gym.Env):\n",
    "    def __init__(self, scenario_path, frame_skip=4):\n",
    "        super(VizdoomEnv, self).__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(scenario_path)\n",
    "        self.game.set_doom_game_path(LOCAL_WAD_PATH)\n",
    "        self.game.set_window_visible(False)\n",
    "        self.game.set_mode(Mode.PLAYER)\n",
    "        self.game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        self.game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "        self.game.init()\n",
    "\n",
    "        self.frame_skip = frame_skip\n",
    "        self.action_space = spaces.Discrete(self.game.get_available_buttons_size())\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.game.get_screen_height(), self.game.get_screen_width(), 1), dtype=np.uint8)\n",
    "\n",
    "        self.previous_game_variables = None\n",
    "\n",
    "    def step(self, action):\n",
    "        buttons = np.zeros(self.game.get_available_buttons_size())\n",
    "        buttons[action] = 1\n",
    "\n",
    "        reward = self.game.make_action(buttons.tolist(), self.frame_skip)\n",
    "        done = self.game.is_episode_finished()\n",
    "\n",
    "        state = self.game.get_state().screen_buffer if not done else np.zeros(self.observation_space.shape, dtype=np.uint8)\n",
    "        state = np.expand_dims(state, axis=-1)\n",
    "\n",
    "        shaped_reward = reward + self._shape_reward()\n",
    "\n",
    "        return state, shaped_reward, done, False, {}\n",
    "\n",
    "    def _shape_reward(self):\n",
    "        current_game_vars = self.game.get_state().game_variables if self.game.get_state() else None\n",
    "        reward = 0\n",
    "\n",
    "        if current_game_vars is None or self.previous_game_variables is None:\n",
    "            self.previous_game_variables = current_game_vars\n",
    "            return reward\n",
    "\n",
    "        reward += (current_game_vars[0] - self.previous_game_variables[0]) * 100.0\n",
    "        reward -= (self.previous_game_variables[2] - current_game_vars[2]) * 0.1\n",
    "        reward -= (self.previous_game_variables[1] - current_game_vars[1])\n",
    "        reward += 0.1\n",
    "\n",
    "        min_dist_now = self._get_closest_enemy_distance()\n",
    "        if hasattr(self, 'min_dist_prev'):\n",
    "            if min_dist_now < self.min_dist_prev and min_dist_now < 500:\n",
    "                reward += 0.05\n",
    "            elif min_dist_now > self.min_dist_prev and self.min_dist_prev < 500:\n",
    "                reward -= 0.05\n",
    "        self.min_dist_prev = min_dist_now\n",
    "\n",
    "        self.previous_game_variables = current_game_vars\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _get_closest_enemy_distance(self):\n",
    "        min_dist = float('inf')\n",
    "        current_game_vars = self.game.get_state().game_variables if self.game.get_state() else None\n",
    "\n",
    "        if current_game_vars is None:\n",
    "            return min_dist\n",
    "\n",
    "        px, py = current_game_vars[3], current_game_vars[4]\n",
    "\n",
    "        for obj in self.game.get_state().objects:\n",
    "            if obj.is_enemy():\n",
    "                dist = ((px - obj.position_x)**2 + (py - obj.position_y)**2)**0.5\n",
    "                min_dist = min(min_dist, dist)\n",
    "\n",
    "        return min_dist\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        state = np.expand_dims(state, axis=-1)\n",
    "        self.previous_game_variables = None\n",
    "        self.min_dist_prev = float('inf')\n",
    "        return state, {}\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "# Unit tests for VizdoomEnv\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestVizdoomEnv(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.env = VizdoomEnv(LOCAL_SCENARIO_PATH)\n",
    "\n",
    "    def test_initialization(self):\n",
    "        self.assertIsInstance(self.env, gym.Env)\n",
    "        self.assertIsNotNone(self.env.game)\n",
    "        self.assertEqual(self.env.frame_skip, 4)\n",
    "\n",
    "    def test_step(self):\n",
    "        state, reward, done, _, _ = self.env.step(0)\n",
    "        self.assertEqual(state.shape, self.env.observation_space.shape)\n",
    "        self.assertIsInstance(reward, float)\n",
    "        self.assertIsInstance(done, bool)\n",
    "\n",
    "    def test_reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.assertEqual(state.shape, self.env.observation_space.shape)\n",
    "\n",
    "    def test_close(self):\n",
    "        self.env.close()\n",
    "        self.assertFalse(self.env.game.is_running())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n",
    "\n",
    "# Initialize environment\n",
    "env = DummyVecEnv([lambda: VizdoomEnv(LOCAL_SCENARIO_PATH)])\n",
    "\n",
    "# Configure PPO\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=f\"{BASE_PATH}/logs\",\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128\n",
    ")\n",
    "\n",
    "# Train\n",
    "TIMESTEPS = 100000\n",
    "model.learn(total_timesteps=TIMESTEPS)\n",
    "model.save(f\"{BASE_PATH}/models/doom_ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent Configuration\n",
    "Configure PPO agent hyperparameters, neural network architecture, and training settings. Implement input validation and error handling for agent setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent Configuration\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch as th\n",
    "\n",
    "# Define PPO hyperparameters with input validation\n",
    "def get_ppo_hyperparameters():\n",
    "    learning_rate = get_user_input(\"Enter the learning rate (e.g., 0.0003): \", type_=float, min_=1e-6, max_=1e-1)\n",
    "    n_steps = get_user_input(\"Enter the number of steps to run for each environment per update (e.g., 2048): \", type_=int, min_=1)\n",
    "    batch_size = get_user_input(\"Enter the batch size (e.g., 64): \", type_=int, min_=1)\n",
    "    n_epochs = get_user_input(\"Enter the number of epochs (e.g., 10): \", type_=int, min_=1)\n",
    "    gamma = get_user_input(\"Enter the discount factor (e.g., 0.99): \", type_=float, min_=0.0, max_=1.0)\n",
    "    gae_lambda = get_user_input(\"Enter the GAE lambda (e.g., 0.95): \", type_=float, min_=0.0, max_=1.0)\n",
    "    clip_range = get_user_input(\"Enter the clip range (e.g., 0.2): \", type_=float, min_=0.0, max_=1.0)\n",
    "    ent_coef = get_user_input(\"Enter the entropy coefficient (e.g., 0.01): \", type_=float, min_=0.0, max_=1.0)\n",
    "    vf_coef = get_user_input(\"Enter the value function coefficient (e.g., 0.5): \", type_=float, min_=0.0, max_=1.0)\n",
    "    max_grad_norm = get_user_input(\"Enter the maximum gradient norm (e.g., 0.5): \", type_=float, min_=0.0, max_=10.0)\n",
    "    return {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"max_grad_norm\": max_grad_norm\n",
    "    }\n",
    "\n",
    "# Get PPO hyperparameters from user\n",
    "ppo_hyperparameters = get_ppo_hyperparameters()\n",
    "logging.info(f\"PPO hyperparameters: {ppo_hyperparameters}\")\n",
    "\n",
    "# Define neural network architecture\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=th.nn.ReLU,\n",
    "    net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
    ")\n",
    "\n",
    "# Create and configure PPO agent\n",
    "try:\n",
    "    env = VizdoomEnv(LOCAL_SCENARIO_PATH)\n",
    "    env = Monitor(env, LOCAL_LOG_PATH)\n",
    "    model = PPO(\n",
    "        \"CnnPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=LOCAL_TENSORBOARD_PATH,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        **ppo_hyperparameters\n",
    "    )\n",
    "    logging.info(\"PPO agent created successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating PPO agent: {e}\")\n",
    "    print(f\"Error creating PPO agent: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=max(10000, ppo_hyperparameters[\"n_steps\"] // 10),\n",
    "    save_path=LOCAL_MODEL_PATH,\n",
    "    name_prefix=\"ppo_vizdoom\"\n",
    ")\n",
    "\n",
    "# Train the PPO agent\n",
    "try:\n",
    "    model.learn(total_timesteps=ppo_hyperparameters[\"n_steps\"], callback=checkpoint_callback)\n",
    "    logging.info(\"PPO agent training completed.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during PPO agent training: {e}\")\n",
    "    print(f\"Error during PPO agent training: {e}\")\n",
    "    if env:\n",
    "        env.close()\n",
    "    exit()\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = os.path.join(LOCAL_MODEL_PATH, \"ppo_vizdoom_final\")\n",
    "try:\n",
    "    model.save(final_model_path)\n",
    "    logging.info(f\"Final PPO model saved to: {final_model_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving final PPO model: {e}\")\n",
    "    print(f\"Error saving final PPO model: {e}\")\n",
    "\n",
    "# Close the environment\n",
    "if env:\n",
    "    env.close()\n",
    "logging.info(\"Environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "Build training loop with checkpointing, monitoring and logging. Implement early stopping and model saving. Add performance optimization and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch as th\n",
    "\n",
    "# Define PPO hyperparameters with input validation\n",
    "def get_ppo_hyperparameters():\n",
    "    learning_rate = get_user_input(\"Enter the learning rate (e.g., 0.0003): \", type_=float, min_=1e-6, max_=1e-1)\n",
    "    n_steps = get_user_input(\"Enter the number of steps to run for each environment per update (e.g., 2048): \", type_=int, min_=1)\n",
    "    batch_size = get_user_input(\"Enter the batch size (e.g., 64): \", type_=int, min_=1)\n",
    "    n_epochs = get_user_input(\"Enter the number of epochs (e.g., 10): \", type_=int, min_=1)\n",
    "    gamma = get_user_input(\"Enter the discount factor (e.g., 0.99): \", type_=float, min_=0.0, max_=1.0)\n",
    "    gae_lambda = get_user_input(\"Enter the GAE lambda (e.g., 0.95): \", type_=float, min_=0.0, max_=1.0)\n",
    "    clip_range = get_user_input(\"Enter the clip range (e.g., 0.2): \", type_=float, min_=0.0, max_=1.0)\n",
    "    ent_coef = get_user_input(\"Enter the entropy coefficient (e.g., 0.01): \", type_=float, min_=0.0, max_=1.0)\n",
    "    vf_coef = get_user_input(\"Enter the value function coefficient (e.g., 0.5): \", type_=float, min_=0.0, max_=1.0)\n",
    "    max_grad_norm = get_user_input(\"Enter the maximum gradient norm (e.g., 0.5): \", type_=float, min_=0.0, max_=10.0)\n",
    "    return {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"max_grad_norm\": max_grad_norm\n",
    "    }\n",
    "\n",
    "# Get PPO hyperparameters from user\n",
    "ppo_hyperparameters = get_ppo_hyperparameters()\n",
    "logging.info(f\"PPO hyperparameters: {ppo_hyperparameters}\")\n",
    "\n",
    "# Define neural network architecture\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=th.nn.ReLU,\n",
    "    net_arch=[dict(pi=[64, 64], vf=[64, 64])]\n",
    ")\n",
    "\n",
    "# Create and configure PPO agent\n",
    "try:\n",
    "    env = VizdoomEnv(LOCAL_SCENARIO_PATH)\n",
    "    env = Monitor(env, LOCAL_LOG_PATH)\n",
    "    model = PPO(\n",
    "        \"CnnPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=LOCAL_TENSORBOARD_PATH,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        **ppo_hyperparameters\n",
    "    )\n",
    "    logging.info(\"PPO agent created successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating PPO agent: {e}\")\n",
    "    print(f\"Error creating PPO agent: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=max(10000, ppo_hyperparameters[\"n_steps\"] // 10),\n",
    "    save_path=LOCAL_MODEL_PATH,\n",
    "    name_prefix=\"ppo_vizdoom\"\n",
    ")\n",
    "\n",
    "# Define evaluation callback for early stopping\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=LOCAL_MODEL_PATH,\n",
    "    log_path=LOCAL_LOG_PATH,\n",
    "    eval_freq=max(10000, ppo_hyperparameters[\"n_steps\"] // 10),\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train the PPO agent\n",
    "try:\n",
    "    model.learn(total_timesteps=ppo_hyperparameters[\"n_steps\"], callback=[checkpoint_callback, eval_callback])\n",
    "    logging.info(\"PPO agent training completed.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during PPO agent training: {e}\")\n",
    "    print(f\"Error during PPO agent training: {e}\")\n",
    "    if env:\n",
    "        env.close()\n",
    "    exit()\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = os.path.join(LOCAL_MODEL_PATH, \"ppo_vizdoom_final\")\n",
    "try:\n",
    "    model.save(final_model_path)\n",
    "    logging.info(f\"Final PPO model saved to: {final_model_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving final PPO model: {e}\")\n",
    "    print(f\"Error saving final PPO model: {e}\")\n",
    "\n",
    "# Close the environment\n",
    "if env:\n",
    "    env.close()\n",
    "logging.info(\"Environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Management\n",
    "Implement model saving/loading, version control and backup. Add model evaluation metrics and visualization. Include security validation for model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Management\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch as th\n",
    "\n",
    "# Function to save the model with version control\n",
    "def save_model_with_version(model, save_path, version):\n",
    "    versioned_path = f\"{save_path}_v{version}\"\n",
    "    try:\n",
    "        model.save(versioned_path)\n",
    "        logging.info(f\"Model saved to: {versioned_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving model to {versioned_path}: {e}\")\n",
    "        print(f\"Error saving model to {versioned_path}: {e}\")\n",
    "\n",
    "# Function to load the model with security validation\n",
    "def load_model_with_validation(model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            model = PPO.load(model_path)\n",
    "            logging.info(f\"Model loaded from: {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model from {model_path}: {e}\")\n",
    "            print(f\"Error loading model from {model_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.error(f\"Model path does not exist: {model_path}\")\n",
    "        print(f\"Model path does not exist: {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    avg_reward = sum(episode_rewards) / num_episodes\n",
    "    logging.info(f\"Average reward over {num_episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "# Function to visualize evaluation metrics\n",
    "def visualize_evaluation_metrics(rewards):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Model Evaluation Metrics')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and version\n",
    "    model_save_path = os.path.join(LOCAL_MODEL_PATH, \"ppo_vizdoom\")\n",
    "    model_version = 1\n",
    "\n",
    "    # Save the model with version control\n",
    "    save_model_with_version(model, model_save_path, model_version)\n",
    "\n",
    "    # Load the model with validation\n",
    "    loaded_model = load_model_with_validation(f\"{model_save_path}_v{model_version}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    if loaded_model:\n",
    "        avg_reward = evaluate_model(loaded_model, env)\n",
    "        print(f\"Average reward: {avg_reward}\")\n",
    "\n",
    "        # Visualize evaluation metrics\n",
    "        visualize_evaluation_metrics([avg_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing & Validation\n",
    "Write unit tests for environment, agent and training components. Add integration tests and example usage. Implement validation of trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing & Validation\n",
    "\n",
    "import unittest\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "class TestVizdoomEnv(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.env = VizdoomEnv(LOCAL_SCENARIO_PATH)\n",
    "\n",
    "    def test_initialization(self):\n",
    "        self.assertIsInstance(self.env, gym.Env)\n",
    "        self.assertIsNotNone(self.env.game)\n",
    "        self.assertEqual(self.env.frame_skip, 4)\n",
    "\n",
    "    def test_step(self):\n",
    "        state, reward, done, _, _ = self.env.step(0)\n",
    "        self.assertEqual(state.shape, self.env.observation_space.shape)\n",
    "        self.assertIsInstance(reward, float)\n",
    "        self.assertIsInstance(done, bool)\n",
    "\n",
    "    def test_reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        self.assertEqual(state.shape, self.env.observation_space.shape)\n",
    "\n",
    "    def test_close(self):\n",
    "        self.env.close()\n",
    "        self.assertFalse(self.env.game.is_running())\n",
    "\n",
    "class TestPPOAgent(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.env = VizdoomEnv(LOCAL_SCENARIO_PATH)\n",
    "        self.env = Monitor(self.env, LOCAL_LOG_PATH)\n",
    "        self.model = PPO(\"CnnPolicy\", self.env, verbose=1, tensorboard_log=LOCAL_TENSORBOARD_PATH)\n",
    "\n",
    "    def test_initialization(self):\n",
    "        self.assertIsInstance(self.model, PPO)\n",
    "\n",
    "    def test_training(self):\n",
    "        try:\n",
    "            self.model.learn(total_timesteps=100)\n",
    "            self.assertTrue(True)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Training failed with exception: {e}\")\n",
    "\n",
    "    def test_save_load(self):\n",
    "        model_path = os.path.join(LOCAL_MODEL_PATH, \"test_model\")\n",
    "        self.model.save(model_path)\n",
    "        loaded_model = PPO.load(model_path)\n",
    "        self.assertIsInstance(loaded_model, PPO)\n",
    "\n",
    "class TestModelManagement(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.env = VizdoomEnv(LOCAL_SCENARIO_PATH)\n",
    "        self.env = Monitor(self.env, LOCAL_LOG_PATH)\n",
    "        self.model = PPO(\"CnnPolicy\", self.env, verbose=1, tensorboard_log=LOCAL_TENSORBOARD_PATH)\n",
    "\n",
    "    def test_save_model_with_version(self):\n",
    "        model_save_path = os.path.join(LOCAL_MODEL_PATH, \"ppo_vizdoom\")\n",
    "        model_version = 1\n",
    "        save_model_with_version(self.model, model_save_path, model_version)\n",
    "        self.assertTrue(os.path.exists(f\"{model_save_path}_v{model_version}\"))\n",
    "\n",
    "    def test_load_model_with_validation(self):\n",
    "        model_save_path = os.path.join(LOCAL_MODEL_PATH, \"ppo_vizdoom_v1\")\n",
    "        self.model.save(model_save_path)\n",
    "        loaded_model = load_model_with_validation(model_save_path)\n",
    "        self.assertIsInstance(loaded_model, PPO)\n",
    "\n",
    "    def test_evaluate_model(self):\n",
    "        avg_reward = evaluate_model(self.model, self.env, num_episodes=5)\n",
    "        self.assertIsInstance(avg_reward, float)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
